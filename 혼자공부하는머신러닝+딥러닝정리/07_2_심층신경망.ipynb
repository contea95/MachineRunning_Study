{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07-2_심층신경망.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjmPCZJtK6uu"
      },
      "source": [
        "## 2개의 층\n",
        "케라스 api로 패션 mnist 데이터셋을 불러오겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19-pzfjeKpd_",
        "outputId": "6989b065-d9f2-42f0-818e-54e61e515aca"
      },
      "source": [
        "from tensorflow import keras\n",
        "(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d57INU9KMAvt"
      },
      "source": [
        "픽셀값 범위를 0~1사이, 28x28 크기의 2차원 배열을 784 크기 1차원 배열로 펼치고 사이킷런의 train_test_split() 함수로 훈련 세트와 검증 세트로 나눈다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4P6keI9MBfI"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_scaled = train_input / 255.0\n",
        "train_scaled = train_scaled.reshape(-1, 28*28)\n",
        "train_scaled, val_scaled, train_target, val_target = train_test_split(\n",
        "    train_scaled, train_target, test_size = 0.2, random_state = 42\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgsuQJ-xMlFT"
      },
      "source": [
        "이제 인공 신경망 모델에 층을 2개 추가해보자.\n",
        "\n",
        "입력층과 출력층 사이에 밀집층이 추가된 것이다. 이 사이의 모든 층을 은닉 층이라고 부른다.\n",
        "\n",
        "은닉층에는 주황색 원으로 활성화 함수가 표시되어 있다. 활성화 함수는 신경망 층의 선형 방정식의 계산 값에 적용하는 함수. 소프트맥스 함수도 활성화 함수이다.\n",
        "\n",
        "출력층에 적용하는 활성화 함수는 종류가 제한되어 있다.\n",
        "\n",
        "이진 분류일 경우 시그모이드, 다중 분류일 경우 소프트맥스 함수 사용.\n",
        "\n",
        "은닉층에 왜 활성화 함수를 적용할까? 다음 2개의 선형 방정식을 생각해보자.\n",
        "\n",
        "    a * 4 + 2 = b\n",
        "                    ->a * 12 + 1 = c                \n",
        "    b * 3 - 5 = c\n",
        "\n",
        "왼쪽의 첫 번째 식에서 계산된 b가 두 번째 식에서 c를 계산하기 위해 쓰인다. 하지만 두 번째 식에 첫번째 식을 대입하면 하나로 합쳐진다. 이렇게 되면 b는 사라지면서 b가 하는 일이 없어지는 셈이다.\n",
        "\n",
        "신경망도 마찬가지로 은닉층에서 선형적인 산술 계산만 수행하면 수행 역할이 없는 셈이다. 선형 계산을 적당하게 비 선형적으로 비틀어 주어야 다음 층의 계산과 단순히 합쳐지지 않고 나름의 역할을 할 수 있다.\n",
        "\n",
        "시그모이드 함수는 뉴런의 출력 z값을 0과 1 사이로 압축한다. 이 시그모이드 활성화 함수를 사용한 은닉층과 소프트 맥스 함수를 사용한 출력층을 케라스의 Dense 클래스로 만들어 보자.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GS737D2R2O1"
      },
      "source": [
        "dense1 = keras.layers.Dense(100, activation='sigmoid',input_shape=(784,))\n",
        "dense2 = keras.layers.Dense(10, activation = 'softmax')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VId35IDsSEKe"
      },
      "source": [
        "dense1이 은닉층이고 100개의 뉴런을 가진 밀집층. 활성화 함수를 sigmoid로 지정, input_shape 매개변수에서 입력의 크기를 지정.\n",
        "여기서 은닉층의 뉴런 개수를 정하는 데는 기준이 없다.\n",
        "\n",
        "여기서 한가지 제약사항은 적어도 출력층의 뉴런보다는 많게 만들어야 한다는 것이다. 클래스 10개에 대한 확률을 예측해야 하는데 이전 은닉층의 뉴런이 10개보다 적으면 부족한 정보가 전달된다.\n",
        "\n",
        "dense2는 출력층. 10개의 클래스를 분류하므로 10개의 뉴런을 두고 활성화 함수는 소프트 맥스 함수로 지정.\n",
        "\n",
        "## 심층 신경망 만들기\n",
        "dense1, dense2 객체를 Sequential 클래스에 추사하여 심층 신경망을 만들어 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CXctll9StFs"
      },
      "source": [
        "model = keras.Sequential([dense1, dense2])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc4lZCmsSx4J"
      },
      "source": [
        "Sequential 클래스의 객체를 만들 때 여러 개의 층을 추가하려면 층 객체를 리스트로 전달한다. 여기서 출력층을 가장 마지막에 두어야 한다.\n",
        "\n",
        "케라스는 모델의 summary() 메서드를 호출하면 층에 대한 정보를 얻을 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LpjmP5RTCdA",
        "outputId": "a62d3591-43f2-456d-f96b-4eaa3918d749"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybyS4eONTIFD"
      },
      "source": [
        "첫줄에는 모델 이름, 은닉층, 출력층 순서대로 나옴\n",
        "\n",
        "층을 만들 때 name 매개변수로 이름을 지정할 수 있다.\n",
        "\n",
        "출력 크기는 (None, 100)이다. None는 샘플의 개수를 나타낸다. 샘플의 개수가 정해지지 않아서이다. 케라스 모델의 fit() 메서드에 훈련 데이터를 주입하면 이 데이터를 한번에 사용하지 않고 잘게 나누어 여러번에 걸쳐 경사 하강법 단계를 수행한다.\n",
        "\n",
        "케라스의 기본 미니 배치 크기는 32개이다. 이 값은 fit() 메서드에서 batch_size 매개변수로 바꿀 수 있다. 샘플 개수를 고정하지 않고 어떤 배치 크기에도 유연하게 대응하도록 None으로 설정.\n",
        "\n",
        "두번째 100은 뉴런의 개수\n",
        "\n",
        "마지막으로 모델의 파라미터 개수가 출력. 이 층은 Dense 층이므로 입력 픽셀의 784개와 100개의 모든 조합에 대한 가중치. 그리고 뉴런마다 1개의 절편이 있다.\n",
        "\n",
        "summary() 메서드의 마지막에는 총 모델 파라미터 개수와 훈련되는 파라미터 개수가 동일하게 나온다. 은닉층과 출력층의 파라미터 개수를 합친 것이다. 그 아래는 훈련되지 않는 파라미터는 0으로 나온다.\n",
        "\n",
        "## 층을 추가하는 다른 방법\n",
        "처음에는 Dense클래스의 객체 1,2를 만들어 Sequential클래스에 전달했다. 이 두 객체는 저장하여 쓸 일이 없기 때문에 Sequential 클래스의 생성자 안에서 바로 Dense 클래스의 객체를 만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuFvIjL_V-OH"
      },
      "source": [
        "model = keras.Sequential([\n",
        "                          keras.layers.Dense(100, activation = 'sigmoid', input_shape = (784,),name = 'hidden'),\n",
        "                          keras.layers.Dense(10, activation = 'softmax', name = 'output')\n",
        "], name = '패션 MNIST 모델')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmjI7dHAWf8T",
        "outputId": "4b562df0-1e8a-4b6a-ca53-de14ece61a3e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"패션 MNIST 모델\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "hidden (Dense)               (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoYMNrHWkZq"
      },
      "source": [
        "많은 층을 추가하려면 모델의 add() 메서드를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1pBhP11WpSp"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr3tLn63W4PQ",
        "outputId": "ae29984d-46c7-4083-dbc4-3f2609295cd2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd72GZjuXScI"
      },
      "source": [
        "이제 모델을 훈련해보자. compile()메서드 설정은 동일하게 한다 5번의 에포크 동안 훈련.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch_U4ZpFXX6X",
        "outputId": "27885974-c15f-499a-ba07-19d36cda4551"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',metrics = 'accuracy')\n",
        "model.fit(train_scaled, train_target, epochs=5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.7631 - accuracy: 0.7566\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4108 - accuracy: 0.8529\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3718 - accuracy: 0.8655\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3534 - accuracy: 0.8729\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3285 - accuracy: 0.8816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff6117d21d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoR1KZ1lZkac"
      },
      "source": [
        "훈련 세트에 대한 성능을 보면 추가된 층이 성능을 향상시켰다.\n",
        "\n",
        "다음에는 이미지 분류 문제에서 높은 성능을 낼 수 있는 활성화 함수에 대해 알아보자.\n",
        "\n",
        "## 렐루 함수\n",
        "렐루 함수는 간단하다. 입력이 양수일 경우 활성화 함수가 없는 것처럼 입력을 통과시키고 음수일 경우에는 0으로 만든다.\n",
        "\n",
        "렐루 함수는 max(0,z)와 같이 쓸 수 있다. z가 0보다 크면 z를 출력하고 z가 0보다 작으면 0을 출력한다.\n",
        "\n",
        "패션MNIST 데이터는 28*28크기이기 때문에 인공 신경망에 주입하기 위해 넘파이 배열의 reshape() 메서드를 통해 1차원으로 펼쳤다. 케라스에서는 이를 위한 Flatten층을 제공한다.\n",
        "\n",
        "Flatten 클래스는 배치 차원을 제외하고 나머지 입력 차원을 모두 일렬로 펼치는 역할만 한다. 따라서 인공 신경망의 성능을 위해 기여하는 바는 없지만 Flatten 클래스를 층처럼 입력층과 은닉층 사이에 추가하기 때문에 이를 층이라고 부른다. Flatten층은 입력층 바로 뒤에 추가한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWm6TkM4htql"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
        "model.add(keras.layers.Dense(100, activation='relu'))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSj49Sdxizpk",
        "outputId": "8261e561-c5a6-4ad4-aff3-95b16ff95323"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv77aYqPijvY"
      },
      "source": [
        "(train_input, train_target), (test_input, test_target) =\\\n",
        "    keras.datasets.fashion_mnist.load_data()\n",
        "train_scaled = train_input/255.0\n",
        "train_scaled, val_scaled, train_target, val_target = train_test_split(\n",
        "    train_scaled, train_target, test_size=0.2, random_state=42\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eahn-Qp4l_o0",
        "outputId": "dc220167-bf91-4791-cb04-7682546f2dae"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
        "model.fit(train_scaled, train_target, epochs=10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6775 - accuracy: 0.7667\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3985 - accuracy: 0.8580\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3569 - accuracy: 0.8729\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3329 - accuracy: 0.8806\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3189 - accuracy: 0.8861\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3099 - accuracy: 0.8918\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3015 - accuracy: 0.8920\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2907 - accuracy: 0.8982\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2753 - accuracy: 0.9022\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2759 - accuracy: 0.9044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff60f0ce4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4RVytMspxR_",
        "outputId": "fa10d9c6-1d6d-45b5-a2d8-fb1391e26e26"
      },
      "source": [
        "model.evaluate(val_scaled, val_target)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "375/375 [==============================] - 1s 1ms/step - loss: 0.3863 - accuracy: 0.8757\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3863057494163513, 0.8756666779518127]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXLKgUzKnnCB"
      },
      "source": [
        "## 옵티마이저\n",
        "사람이 지정해주어야 하는 파라미터 = 하이퍼 파라미터\n",
        "\n",
        "여러개의 은닉층을 더 추가할 수도 있다. 추가할 은닉층의 개수는 모델이 학습하는 것이 아니라 우리가 지정해야 한다.\n",
        "\n",
        "케라스의 기본적으로 미니배치 경사 하강법을 사용하며 미니배치 개수는 32개이다. fit()메서드의 batch_size 매개변수에서 이를 조정할 수 있으며 이 역시 하이퍼 파라미터이다. 또한 fit() 메서드의 epochs 매개변수도 하이퍼 파라미터 이다. 반복 횟수에 따라 다른 모델이 만들어지기 때문이다.\n",
        "\n",
        "마지막으로 compile() 메서드에서는 케라스의 기본 경사 하강법 알고리즘인 RMSprop을 사용한다. 케라스는 다양한 종류의 경사 하강법 알고리즘을 제공하고 이를 옵티마이저라고 부른다.\n",
        "\n",
        "가장 기본적인 옵티마이저는 확률적 경사 하강법인 SGD이다. 1개의 샘플을 뽑아서 훈련하지 않고 앞서 언급한 것처럼 기본적으로 미니배치를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r74pWu7irsGd"
      },
      "source": [
        "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epSdym2nsOSh"
      },
      "source": [
        "이 옵티마이저는 tensorflow.keras.optimizers패키지 아래 SGD 클래스로 구현되어 있다. sgd 문자열은 이 클래스의 기본 설정 매개 변수로 생성한 객체와 동일하다. 즉 아래 코드와 동일하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEQz6-iNsN87"
      },
      "source": [
        "sgd = keras.optimizers.SGD()\n",
        "model.compile(optimizer=sgd, loss = 'sparse_categorical_crossentropy', metrics='accuracy')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbi-L0IisrX4"
      },
      "source": [
        "옵티마이저에 'sgd'를 지정하면 자동으로 SGD 클래스 객체를 만들어준다.\n",
        " 또한 SGD클래스의 학습률(0.01)을 바꾸고 싶을 때는 다음과 같은 learning_rate 매개변수에 지정하여 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR3pi8Lhs6Ya"
      },
      "source": [
        "sgd = keras.optimizers.SGD(learning_rate=0.1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1GmBFAvtA4a"
      },
      "source": [
        "SGD외에 다양한 옵티마이저들이 있다.\n",
        "* 기본 경사 하강법 옵티마이저\n",
        "        SGD, 모멘텀, 네스테로프 모멘텀\n",
        "* 적응적 학습률 옵티마이저\n",
        "        RMSprop, Adam, Adagrad\n",
        "\n",
        "기본 경사 하강법 옵티마이저는 모두 SGD 클래스에서 제공. SGD클래스의 momentum 매개변수의 기본 값은 0이다. 이를 0보다 큰 값으로 지정하면 이전 그레이디언트를 가속도처럼 사용하는 모멘텀 최적화를 사용한다. 보통 momentum 매개변수는 0.9이상을 지정한다.\n",
        "\n",
        "다음처럼 SGD 클래스의 nesterov 매개변수를 기본값 False에서 True로 바꾸면 네스테로프 모멘텀 최적화(네스테로프 가속 경사)를 사용한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OvxVhy5uFVX"
      },
      "source": [
        "sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2nEpWKBuByk"
      },
      "source": [
        "네스테로프 모멘텀은 모멘텀 최적화를 2번 반복하여 구현. 대부분 경우 네스테로프 모멘텀 최적화가 기본 확률적 경사 하강법보다 더 나은 성능을 제공한다.\n",
        "\n",
        "모델이 최적점에 가까이 갈수록 학습률을 낮출 수 있다. 이렇게 하면 안정적이고 최적점에 수렴할 가능성이 높다. 이런 학습률을 적응적 학습률이라고 한다.\n",
        "\n",
        "적응적 학습률을 대표하는 옵티마이저는 Adagrad와 RMSprop이다. 각각 compile() 메서드의 optimizer 매개변수에 'adagrad'와 'rmsprop'으로 지정할 수 있다. optimizer 매개변수의 기본값이 바로 'rmsprop'이다. 두 옵티마이저의 매개변수를 바꾸고 싶으면 SGD와 같이 Adagrad와 RMSprop 클래스 객체를 만들어 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTShSh4vw0yR"
      },
      "source": [
        "adagrad = keras.optimizers.Adagrad()\n",
        "model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ajb6Mv-wyp5"
      },
      "source": [
        "rmsprop = keras.optimizers.RMSprop()\n",
        "model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTdJQPJ0xNnK"
      },
      "source": [
        "모멘텀 최적화와 RMSprop의 장점을 접목한 것이 Adam이다. 이는 keras.optimizers 패키지 아래에 모두 있다. 적응적 학습률을 사용하는 이 3개의 클래스는 learning_rate 매개변수의 기본값으로 모두 0.001을 사용한다.\n",
        "\n",
        "Adam 클래스의 매개변수 기본값을 사용해 패션 MNIST모델을 훈련해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdSnS1FkyJXP"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
        "model.add(keras.layers.Dense(100, activation='relu'))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NxdnEzLyfD2",
        "outputId": "457c4ed1-2440-4a30-ff00-4c38a0e1eabe"
      },
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
        "model.fit(train_scaled, train_target, epochs=5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6671 - accuracy: 0.7669\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3965 - accuracy: 0.8573\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3550 - accuracy: 0.8742\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3246 - accuracy: 0.8802\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3009 - accuracy: 0.8903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff60db72588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB3t6X-OyvdF",
        "outputId": "8cac93eb-236a-4829-edd2-80380646c966"
      },
      "source": [
        "model.evaluate(val_scaled, val_target)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "375/375 [==============================] - 1s 1ms/step - loss: 0.3526 - accuracy: 0.8770\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3525923788547516, 0.8769999742507935]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}